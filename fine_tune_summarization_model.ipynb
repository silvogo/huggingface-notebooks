{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPI6htvNehiVpM9PF8U+S9i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TjrtcehK9Nin"},"outputs":[],"source":["!pip install datasets evaluate transformers rouge-score nltk"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"2rOGreRo9bQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! apt install git-lfs"],"metadata":{"id":"7c9lpxtG9uts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"id":"WNo30rjVL0Bx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers.utils import send_example_telemetry\n","send_example_telemetry('summarization_notebook', framework = 'pytorch')"],"metadata":{"id":"uAOXU0sKL5_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = \"t5-small\""],"metadata":{"id":"AP5hAdJTMTPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Loading the dataset"],"metadata":{"id":"tB0-Z9RfMuub"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from evaluate import load\n","\n","raw_datasets = load_dataset(\"xsum\")\n","metric = load(\"rouge\")\n","\n","print(raw_datasets)"],"metadata":{"id":"jhbOvG-AMwpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Acess to an entry point of the training set\n","raw_datasets[\"train\"][0]"],"metadata":{"id":"WfwcNy7gUg1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_datasets[\"train\"][10]"],"metadata":{"id":"whrbj4mSZW8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","import random\n","import pandas as pd\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples = 5):\n","  assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset\"\n","  picks = []\n","\n","  for _ in range(num_examples):\n","    pick = random.randint(0, len(dataset) -1 )\n","    # If the randomly generated pick is already in the picks list (i.e., it’s a duplicate),\n","    # it generates a new random number until it finds one that hasn’t been picked yet\n","    while pick in picks:\n","      pick = random.randint(0, len(dataset) -1)\n","    picks.append(pick)\n","\n","    df = pd.DataFrame(dataset[picks])\n","    # Iterate through each column and its data type in the dataset's features\n","    for column, typ in dataset.features.items():\n","       # Check if the column type is a 'ClassLabel' (i.e., a column with integer labels mapped to class names)\n","      if isinstance(typ, datasets.ClassLabel):\n","         # Transform the column by mapping each integer label to its corresponding class name\n","         # 'typ.names' contains the list of class names, so 'i' is used to map the integer index to the class name\n","        df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))\n"],"metadata":{"id":"brqJTSVeUurF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_random_elements(raw_datasets[\"train\"])"],"metadata":{"id":"bNfhZpNsXoyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric"],"metadata":{"id":"vYb5GuOZXtf8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test rouge metric\n","fake_preds = [\"hello there\", \"general kenobi\"]\n","fake_labels = [\"hello there\", \"general kenobi\"]\n","metric.compute(predictions=fake_preds, references=fake_labels)"],"metadata":{"id":"2eD5rHr-aHn6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocesing the data"],"metadata":{"id":"fdCRO6LKbpiO"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# testing the tokenizer\n","tokenizer(\"Hello this is one sentence\")"],"metadata":{"id":"0Y7qWG8qbHNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer([\"Hello, this one sentence!\", \"This is another sentence\"])"],"metadata":{"id":"g7_qQxrzcNxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer(text_target= [\"Hello, this one sentence!\", \"This is another sentence\"]))"],"metadata":{"id":"4KAkFUi_cdub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n","    prefix = \"summarize: \"\n","else:\n","    prefix = \"\""],"metadata":{"id":"JUhvAoSDcmlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_input_length = 1024\n","max_target_length = 128\n","\n","def preprocess_function(examples):\n","  inputs = [prefix + doc for doc in examples[\"document\"]]\n","  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","  labels = tokenizer(text_target = examples['summary'], max_length = max_target_length, truncation= True)\n","\n","  model_inputs[\"labels\"] = labels[\"input_ids\"]\n","  return model_inputs\n"],"metadata":{"id":"G9S8gOA4d7Ng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocess_function(raw_datasets['train'][:2])"],"metadata":{"id":"jwjN-9gbhAR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets = raw_datasets.map(preprocess_function, batched= True)"],"metadata":{"id":"Sr9GnhVy1KOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tuning the model"],"metadata":{"id":"K3JNjBqL6Kwe"}},{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"],"metadata":{"id":"B73F900H6MXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","model_name = model_checkpoint.split('/')[-1]\n","args = Seq2SeqTrainingArguments(\n","    f\"{model_name}-finetuned-xsum\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate = 2e-5,\n","    per_device_train_batch_size = batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay = 0.01,\n","    save_total_limit = 3,\n","    num_train_epochs=1,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True,\n","    report_to=None\n","\n",")"],"metadata":{"id":"gIQyRQcW91yG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"],"metadata":{"id":"XfsUG-iSMlZY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","  # Eval_pred is a tuple: eval_pred = (predictions, labels)\n","  predictions, labels = eval_pred\n","  decoded_preds = tokenizer.batch_decode(predictions, skip_special_characters= True)\n","  # Replace -100 in the labels that correspond to pad tokens. for evaluation we need to convert them to the pad_token_id of the tokenizer\n","  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","  decoded_labels = tokenizer.batch_decode(labels, skip_special_characters= True)\n","\n","  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n","  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for label in decoded_labels]\n","\n","  result = metric.compute(predictions=decoded_preds, references = decoded_labels, use_stemmer= True, use_aggregator = True)\n","  # Extract a few results\n","  result = {key: value * 100 for key, value in result.items()}\n","  # A list of the lengths of the generated predictions (ignoring padding tokens)\n","  prediction_lens = [np.count_nonzero(pred !=  tokenizer.pad_token_id) for pred in predictions ]\n","  # Computes the average length of all predictions\n","  result['gen_lens'] = np.mean(prediction_lens)\n","\n","  # Formats the final result by rounding all metric values to 4 decimal places\n","  return {k: round(v, 4) for k, v in result.items()}\n"],"metadata":{"id":"R81oEZLBNCjV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The final output of the compute metrics function will be something like:\n","\n","{\n","  \"rouge1\": 45.6789,\n","  \"rouge2\": 30.1234,\n","  \"rougeL\": 50.5678,\n","  \"gen_lens\": 25.4\n","}"],"metadata":{"id":"h8erNmZZTy-8"}},{"cell_type":"code","source":["trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator = data_collator,\n","    tokenizer = tokenizer,\n","    compute_metrics = compute_metrics\n",")"],"metadata":{"id":"C6pO9gMnUBdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""],"metadata":{"id":"s1V551JYiX3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"DwRu8kDhUf7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub('diosilva/my-summarization-model')"],"metadata":{"id":"6ak-hlXxUjQn"},"execution_count":null,"outputs":[]}]}